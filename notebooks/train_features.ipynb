{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Demo Model\n",
    "\n",
    "This notebook shows an example of how to train, evaluate, and save a model to predicted expected goals from match and stadium data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from haxml.utils import (\n",
    "    get_matches_metadata,\n",
    "    get_stadiums,\n",
    "    get_opposing_goalpost,\n",
    "    load_match,\n",
    "    is_target_stadium,\n",
    "    is_scored_goal,\n",
    "    total_scored_goals,\n",
    "    total_kicks,\n",
    "    goal_fraction,\n",
    "    stadium_distance,\n",
    "    angle_from_goal,\n",
    "    train_test_split_matches_even_count\n",
    ")\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Data\n",
    "\n",
    "Choosing a train/test split is an important aspect of training ML models.\n",
    "\n",
    "- We want to have enough data to train and test our model.\n",
    "- We want both splits to be representative of what the model will encounter in production.\n",
    "- We want the target variable to have similar frequency in both the train and test data.\n",
    "- We do not want to leak information about the test data into the train data.\n",
    "\n",
    "Read, execute, and understand how we make this train/test split in the cells below. What are some things that are good and bad about how we are splitting the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadiums = get_stadiums(\"../data/stadiums.json\")\n",
    "metadata = get_matches_metadata(\"../data/matches_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full, inflated match records contain the players' positional data, which can become quite large if we load them all at once. To be more efficient with memory, we have a file of match metadata records, providing us basic information that we can use to split train/test data and to evaluate models, without loading the full match data.\n",
    "\n",
    "The memory savings allow us to do this analysis from the comfort of a Jupyter notebook without needing much more computing power.\n",
    "\n",
    "Check out an example of the metadata for one match. Note that \"scored goals\" can differ from the actual score. We don't consider own-goals as scored goals (only errors, which are shots from an offensive player that deflect off of defenders), since the offense did not directly produce them.\n",
    "\n",
    "We will build a model to predict offensive XG. But XG is also used as a defensive metric. Own goals are one reason a defense may give up more goals than XG predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match_id': '-MPG8aVg406bTT1daesR',\n",
       " 'stadium': 'NAFL 1v1/2v2 Map v1',\n",
       " 'time': 106.8,\n",
       " 'kicks_red': 21,\n",
       " 'kicks_blue': 30,\n",
       " 'score_red': 3,\n",
       " 'score_blue': 2,\n",
       " 'scored_goals_red': 2,\n",
       " 'scored_goals_blue': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `haxml.utils` module has one method for split matches into train and test data. You can check the docstring of a method by calling `help()`. Let's see what this one does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split_matches_even_count in module haxml.utils:\n",
      "\n",
      "train_test_split_matches_even_count(metadata)\n",
      "    Evenly splits matches into train and test lists of almost the same size.\n",
      "    Matches are sorted by number of scored goals, in an attempt to evenly\n",
      "    distribute scored goals between the two lists.\n",
      "    Args:\n",
      "        metadata: List of dicts with IDs and metadata for each match, to split\n",
      "            into train and test lists.\n",
      "    Returns:\n",
      "        (train, test): Tuple containing match IDs for train and test splits\n",
      "            (lists of strings).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split_matches_even_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split_matches_even_count(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_split(metadata):\n",
    "    \"\"\"\n",
    "    Helper method to summarize a train/test split.\n",
    "    Args:\n",
    "        metadata: Match metadata for one split (list of dicts).\n",
    "    \"\"\"\n",
    "    goals = sum(total_scored_goals(m) for m in metadata)\n",
    "    kicks = sum(total_kicks(m) for m in metadata)\n",
    "    frac = goal_fraction(goals, kicks)\n",
    "    print(\"Matches: {:,}\".format(len(metadata)))\n",
    "    print(\"Goals: {:,}\".format(goals))\n",
    "    print(\"Kicks: {:,}\".format(kicks))\n",
    "    print(\"E(XG): {:.3f}\".format(frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Matches: 394\n",
      "Goals: 1,119\n",
      "Kicks: 32,595\n",
      "E(XG): 0.034\n",
      "\n",
      "Test Data:\n",
      "Matches: 393\n",
      "Goals: 1,111\n",
      "Kicks: 32,190\n",
      "E(XG): 0.035\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "summarize_split(train)\n",
    "print()\n",
    "print(\"Test Data:\")\n",
    "summarize_split(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "\n",
    "To make predictions, our model needs some input features. Since we have kicks from different stadiums, we can't use the position where the ball was kicked from on its own.\n",
    "\n",
    "In this demo, we will use some methods from `haxml.utils` to transform the basic kickdata into features that may help predict XG:\n",
    "\n",
    "- `get_opposing_goalpost(stadium, team)`\n",
    "- `stadium_distance(x1, y1, x2, y2)`\n",
    "- `angle_from_goal(x, y, gx, gy)`\n",
    "\n",
    "You can call `help()` to read their docstrings, or check the source code in `haxml/utils.py`.\n",
    "\n",
    "We combine the above methods into a new method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions_at_time(positions, t):\n",
    "    \"\"\"\n",
    "    Return a list of positions (dicts) closest to, but before time t.\n",
    "    \"\"\"\n",
    "    # Assume positions list is already sorted.\n",
    "    # frame is a list of positions (dicts) that have the same timestamp.\n",
    "    frame = []\n",
    "    time = 0.0\n",
    "    for pos in positions:\n",
    "        if pos[\"time\"] > t:\n",
    "            break\n",
    "        if pos[\"time\"] == time:\n",
    "            frame.append(pos)\n",
    "        else:\n",
    "            frame = []\n",
    "            time = pos[\"time\"]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defender_feature(match,kick,dist):\n",
    "    \"\"\"\n",
    "    For a given kick, find the closests defender and the number of defenders within 200 dist\n",
    "    \"\"\"\n",
    "    positions = get_positions_at_time(match[\"positions\"], kick[\"time\"])\n",
    "    ret = [0,0]\n",
    "    closest_defender = float('inf')\n",
    "    defenders_pressuring = 0\n",
    "    for person in positions:\n",
    "        if person['team'] is not kick['fromTeam'] and person['type'] == \"player\": \n",
    "            defender_dist = ((kick['fromX'] - person['x'])**2 + (kick['fromY'] - person['y'])**2)**(1/2) # distance formula\n",
    "            if defender_dist < closest_defender:\n",
    "                closest_defender = defender_dist\n",
    "                ret[0] = closest_defender\n",
    "            if defender_dist <= dist:\n",
    "                defenders_pressuring = defenders_pressuring + 1\n",
    "                ret[1] = defenders_pressuring\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rows_demo(match, stadium):\n",
    "    \"\"\"\n",
    "    Generates target and features for each kick in the match.\n",
    "    Produces two features for demo classifiers:\n",
    "        goal_distance: Distance from where  ball was kicked to goal midpoint.\n",
    "        goal_angle: Angle (in radians) between straight shot from where ball was\n",
    "            kicked to goal midpoint.\n",
    "    Args:\n",
    "        match: Inflated match data (dict).\n",
    "        stadium: Stadium data (dict).\n",
    "    Returns:\n",
    "        Generator of dicts with values for each kick in the given match.\n",
    "        Includes prediction target \"ag\" (actual goals) which is 1 for a scored\n",
    "        goal (goal or error) and 0 otherwise, \"index\" which is the index of the\n",
    "        kick in the match kick list, and all the other features needed for\n",
    "        prediction and explanation.\n",
    "    \"\"\"\n",
    "    for i, kick in enumerate(match[\"kicks\"]):\n",
    "        gp = get_opposing_goalpost(stadium, kick[\"fromTeam\"])\n",
    "        x = kick[\"fromX\"]\n",
    "        y = kick[\"fromY\"]\n",
    "        gx = gp[\"mid\"][\"x\"]\n",
    "        gy = gp[\"mid\"][\"y\"]\n",
    "        dist = stadium_distance(x, y, gx, gy)\n",
    "        angle = angle_from_goal(x, y, gx, gy)\n",
    "        closest_defender,defender_within = defender_feature(match,kick,100)\n",
    "        row = {\n",
    "            \"ag\": 1 if is_scored_goal(kick) else 0,\n",
    "            \"index\": i,\n",
    "            \"time\": kick[\"time\"],\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"goal_x\": gx,\n",
    "            \"goal_y\": gy,\n",
    "            \"goal_distance\": dist,\n",
    "            \"goal_angle\": angle,\n",
    "            \"team\": kick[\"fromTeam\"],\n",
    "            \"stadium\": match[\"stadium\"],\n",
    "            \"closest_defender\": closest_defender,\n",
    "            \"defender_within\": defender_within\n",
    "        }\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write one more method to handle efficiently reading the full, inflated match data. This method takes as a callback the `generate_rows_demo(match, stadium)` method. It also has a handy progress bar using the `tqdm` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(metadata, callback, progress=False):\n",
    "    \"\"\"\n",
    "    Transforms match metadata into a DataFrame of records for\n",
    "    each kick, including target label and features.\n",
    "    Args:\n",
    "        metadata: Match metadata (list of dicts).\n",
    "        callback: Method to run on each match to extract kicks.\n",
    "        progress: Whether or not to show progress bar (boolean).\n",
    "    Returns:\n",
    "        DataFrame where each row is a kick record.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    bar = tqdm(metadata) if progress else metadata\n",
    "    for meta in bar:\n",
    "        key = meta[\"match_id\"]\n",
    "        infile = \"../data/packed_matches/{}.json\".format(key)\n",
    "        try:\n",
    "            s = stadiums[meta[\"stadium\"]]\n",
    "            row_gen = load_match(infile, lambda m: callback(m, s))\n",
    "            for row in row_gen:\n",
    "                row[\"match\"] = key\n",
    "                rows.append(row)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [00:15<00:00, 26.20it/s]\n",
      "100%|██████████| 393/393 [00:16<00:00, 24.37it/s]\n"
     ]
    }
   ],
   "source": [
    "d_train = make_df(train, generate_rows_demo, progress=True)\n",
    "d_test = make_df(test, generate_rows_demo, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Now we can use scikit-learn to train and evaluate a demo model. We will use a logistic regression classifier. It will take in the two features we extraced (goal distance and goal angle) and predict an XG value in the range of [0, 1].\n",
    "\n",
    "Normally, if a binary classifier outputs a predicted probability greater than 0.5, the predicted label will be true. However, we know that a very small fraction of kicks results in goals, so we may choose an even higher threshold.\n",
    "\n",
    "For this imbalanced prediction task, accuracy will not be a very useful metric. Precision and recall are helpful, but they focus on binary labels. For XG, we want to calibrate the predicted probability so that higher quality opportunities have higher values, even if they don't result in scored goals.\n",
    "\n",
    "What metrics and techniques can we use to evaluate whether our XG models are fit for the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model(yt, yp):\n",
    "    \"\"\"\n",
    "    Helper method to summarize some prediction metrics.\n",
    "    Args:\n",
    "        yt: Array of true scored goal values.\n",
    "        yp: Array of predicted scored goal values.\n",
    "    \"\"\"\n",
    "    print(\"Accuracy = {:.3f}\".format(accuracy_score(yt, yp)))\n",
    "    print(\"Precision = {:.3f}\".format(precision_score(yt, yp)))\n",
    "    print(\"Recall    = {:.3f}\".format(recall_score(yt, yp)))\n",
    "    print(\"ROC AUC   = {:.3f}\".format(roc_auc_score(yt, yp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"closest_defender\",\"goal_distance\",\"goal_angle\"]\n",
    "X_train = d_train[features]\n",
    "y_train = d_train[\"ag\"]\n",
    "X_test = d_test[features]\n",
    "y_test = d_test[\"ag\"]\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_features(features,classifier,kwargs):\n",
    "    X_train = d_train[features]\n",
    "    y_train = d_train[\"ag\"]\n",
    "    X_test = d_test[features]\n",
    "    y_test = d_test[\"ag\"]\n",
    "    clf = classifier(**kwargs)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #print(\"Train Scores:\")\n",
    "    #summarize_model(y_train, clf.predict(X_train))\n",
    "    #print()\n",
    "    print(\"Test Scores:\")\n",
    "    summarize_model(y_test, clf.predict(X_test))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores:\n",
      "Accuracy = 0.966\n",
      "Precision = 0.500\n",
      "Recall    = 0.018\n",
      "ROC AUC   = 0.509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"closest_defender\",\"defender_within\",\"goal_distance\",\"goal_angle\"]\n",
    "clf = model_features(features, LogisticRegression, {\"random_state\": 0})\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "features = [\"closest_defender\",\"defender_within\",\"goal_distance\",\"goal_angle\"]\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(2,len(s)+1))\n",
    "\n",
    "results = list(powerset(features))\n",
    "\n",
    "for res in results:\n",
    "    feat = list(res)\n",
    "    #print(feat)\n",
    "    #clf = model_features(feat, LogisticRegression, {\"random_state\": 0})\n",
    "    # easy way to check all combinations\n",
    "    # with distance within set to 100, the best model based on precision is all 4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While traditional evaluation metrics focus on the record-level, we also want to aggregate the XG scores to the match level and compare the actual scored goals to our XG for each team.\n",
    "\n",
    "We could also aggregate XG by player, or compare offensive XG to allowed goals for defensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ag</th>\n",
       "      <th>xg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <th>team</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">-MOTVkwbfE_IKa15MVn9</th>\n",
       "      <th>blue</th>\n",
       "      <td>1</td>\n",
       "      <td>0.140844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>2</td>\n",
       "      <td>0.210644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-MOy-f6_nveB6alhv7BD</th>\n",
       "      <th>red</th>\n",
       "      <td>1</td>\n",
       "      <td>0.151934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-MOy0mtEf9VvJLYeb9g_</th>\n",
       "      <th>red</th>\n",
       "      <td>1</td>\n",
       "      <td>0.096823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-MOy1YNMaXX-VaR3ROtD</th>\n",
       "      <th>red</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">-MOy_8TXdyiIYo9ty5Zu</th>\n",
       "      <th>blue</th>\n",
       "      <td>0</td>\n",
       "      <td>0.934522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>2</td>\n",
       "      <td>1.562130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">-MOy_aJlP-DUs6MyaR1N</th>\n",
       "      <th>blue</th>\n",
       "      <td>2</td>\n",
       "      <td>0.656871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>3</td>\n",
       "      <td>0.831950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-MOybwFHeoLse3Kyjf3h</th>\n",
       "      <th>red</th>\n",
       "      <td>1</td>\n",
       "      <td>0.179293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ag        xg\n",
       "match                team              \n",
       "-MOTVkwbfE_IKa15MVn9 blue   1  0.140844\n",
       "                     red    2  0.210644\n",
       "-MOy-f6_nveB6alhv7BD red    1  0.151934\n",
       "-MOy0mtEf9VvJLYeb9g_ red    1  0.096823\n",
       "-MOy1YNMaXX-VaR3ROtD red    1  0.008135\n",
       "-MOy_8TXdyiIYo9ty5Zu blue   0  0.934522\n",
       "                     red    2  1.562130\n",
       "-MOy_aJlP-DUs6MyaR1N blue   2  0.656871\n",
       "                     red    3  0.831950\n",
       "-MOybwFHeoLse3Kyjf3h red    1  0.179293"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = clf.predict_proba(X_test)[:,1]\n",
    "df_results = pd.DataFrame(d_test)\n",
    "df_results[\"xg\"] = p_test\n",
    "df_results.groupby([\"match\", \"team\"])[[\"ag\", \"xg\"]].sum().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "This demo model is unrefined, and by the measures we briefly looked at, it's not good. Still, it gives us a place to start from.\n",
    "\n",
    "We can use `joblib` to save this model. We will commit this model to our repository. If you have in-progress models, write those to a folder you have gitignored to avoid committing them.\n",
    "\n",
    "We can now use this model in our server to produce on-demand predictions for new matches. The helper function `predict_xg(match, stadium, clf)` takes the new match, the stadium data, and our stored classifier and then augments the match response with XG predictions.\n",
    "\n",
    "We will also save the new methods we wrote in this notebook so we can use them in our server as well. Common utility functions will go in `haxml/utils.py` and data preparation and prediction functions will go in `haxml/prediction.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/demo_logistic_regression.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf, \"../models/demo_logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xg_demo(match, stadium, generate_rows, clf):\n",
    "    \"\"\"\n",
    "    Augments match data with XG predictions.\n",
    "    Args:\n",
    "        match: Inflated match data (dict).\n",
    "        stadium: Stadium data (dict).\n",
    "        generate_rows: function(match, stadium) to generate kick records.\n",
    "        clf: Classifier following scikit-learn interface.\n",
    "    Returns:\n",
    "        Inflated match data with \"xg\" field added to each kick (dict).\n",
    "    \"\"\"\n",
    "    features = [\"goal_distance\", \"goal_angle\"]\n",
    "    d_kicks = pd.DataFrame(generate_rows(match, stadium))\n",
    "    d_kicks[\"xg\"] = clf.predict_proba(d_kicks[features])[:,1]\n",
    "    for kick in d_kicks.to_dict(orient=\"records\"):\n",
    "        match[\"kicks\"][kick[\"index\"]][\"xg\"] = kick[\"xg\"]\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match_id': '-MPGp7v2WuaUTIuQUOZ9',\n",
       " 'stadium': 'NAFL 1v1/2v2 Map v1',\n",
       " 'time': 75.9,\n",
       " 'kicks_red': 11,\n",
       " 'kicks_blue': 11,\n",
       " 'score_red': 3,\n",
       " 'score_blue': 0,\n",
       " 'scored_goals_red': 2,\n",
       " 'scored_goals_blue': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta = test[45]\n",
    "s = stadiums[test_meta[\"stadium\"]]\n",
    "demo_clf = joblib.load(\"../models/demo_logistic_regression.pkl\")\n",
    "test_match = load_match(\n",
    "    \"../data/packed_matches/{}.json\".format(test_meta[\"match_id\"]),\n",
    "    lambda m: predict_xg_demo(m, s, generate_rows_demo, demo_clf)\n",
    ")\n",
    "test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>fromId</th>\n",
       "      <th>fromX</th>\n",
       "      <th>fromY</th>\n",
       "      <th>fromName</th>\n",
       "      <th>fromTeam</th>\n",
       "      <th>toId</th>\n",
       "      <th>toX</th>\n",
       "      <th>toY</th>\n",
       "      <th>toName</th>\n",
       "      <th>toTeam</th>\n",
       "      <th>xg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61.7</td>\n",
       "      <td>goal</td>\n",
       "      <td>4</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Player 82</td>\n",
       "      <td>red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.074480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75.9</td>\n",
       "      <td>goal</td>\n",
       "      <td>4</td>\n",
       "      <td>200.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>Player 82</td>\n",
       "      <td>red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.229034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time  type  fromId  fromX  fromY   fromName fromTeam  toId  toX  toY  \\\n",
       "17  61.7  goal       4  -46.0   56.0  Player 82      red   NaN  NaN  NaN   \n",
       "21  75.9  goal       4  200.0  -63.0  Player 82      red   NaN  NaN  NaN   \n",
       "\n",
       "   toName toTeam        xg  \n",
       "17   None   None  0.074480  \n",
       "21   None   None  0.229034  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_match[\"kicks\"]).query(\"type == 'goal'\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
